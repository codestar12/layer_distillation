{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "num_predictions = 20\n",
    "\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model = load_model('./refactor_finetune.h5')\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.7634 - accuracy: 0.8167\n",
      "Test loss: 0.7633962971687317\n",
      "Test accuracy: 0.8167\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_18 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_16 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_14 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_12 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_10 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_8 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_6 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_4 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_2 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_1 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_22 (Separab (None, 32, 32, 64)        4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_22 (B (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_22 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_23 (Separab (None, 32, 32, 64)        4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_23 (B (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_23 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_20 (Separab (None, 16, 16, 128)       8896      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_20 (B (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_20 (ReLU)              (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_21 (Separab (None, 16, 16, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_21 (B (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_21 (ReLU)              (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_18 (Separab (None, 16, 16, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_18 (B (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_18 (ReLU)              (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_19 (Separab (None, 16, 16, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_19 (B (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_19 (ReLU)              (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_16 (Separab (None, 8, 8, 256)         34176     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_16 (B (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_16 (ReLU)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_17 (Separab (None, 8, 8, 256)         68096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_17 (B (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_17 (ReLU)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_14 (Separab (None, 8, 8, 256)         68096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_14 (B (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_15 (Separab (None, 8, 8, 256)         68096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_15 (B (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_12 (Separab (None, 8, 8, 256)         68096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_12 (B (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_13 (Separab (None, 8, 8, 256)         68096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_13 (B (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_10 (Separab (None, 4, 4, 512)         133888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_10 (B (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_11 (Separab (None, 4, 4, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_11 (B (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_8 (Separabl (None, 4, 4, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_8 (Ba (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_9 (Separabl (None, 4, 4, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_9 (Ba (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_6 (Separabl (None, 4, 4, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_6 (Ba (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_7 (Separabl (None, 4, 4, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_7 (Ba (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 2, 2, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_5 (Separabl (None, 2, 2, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 2, 2, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 2, 2, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d (SeparableC (None, 2, 2, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 2, 2, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 22,478,410\n",
      "Trainable params: 9,728\n",
      "Non-trainable params: 22,468,682\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [i for i, layer in enumerate(model.layers) if layer.__class__.__name__ == 'Conv2D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "dataset_test = dataset.batch(batch_size)\n",
    "dataset_test = dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_replacement(get_output):\n",
    "    inputs = tf.keras.Input(shape=get_output.output[0].shape[1::])\n",
    "    X = tf.keras.layers.SeparableConv2D(filters=get_output.output[1].shape[-1], \n",
    "                                        kernel_size= (3,3),\n",
    "                                        padding='Same')(inputs)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.ReLU()(X)\n",
    "    X = tf.keras.layers.SeparableConv2D(filters=get_output.output[1].shape[-1],\n",
    "                                        kernel_size=(3,3), \n",
    "                                        padding='Same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.ReLU()(X)\n",
    "    replacement_layers = tf.keras.Model(inputs=inputs, outputs=X)\n",
    "    return replacement_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LayerBatch(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, input_model, dataset):\n",
    "        self.input_model = input_model\n",
    "        self.dataset = dataset.__iter__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(50000 / 32)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.input_model(next(self.dataset))\n",
    "        return X, y\n",
    "    \n",
    "import math\n",
    "class LayerTest(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, input_model, dataset):\n",
    "        self.input_model = input_model\n",
    "        self.dataset = dataset.__iter__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(10000 / 32)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.input_model(next(self.dataset))\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets [1, 2, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16, 17]\n",
      "taking target\n",
      "making output for target layer 17\n",
      "making replacement layers for target layer 17\n",
      "starting fit generator for target layer 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-896ca1916920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      verbose=0, callbacks=[save])\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saving replacement layers to json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "import gc\n",
    "while len(targets) > 0:\n",
    "    \n",
    "    print(f'targets {targets}')\n",
    "    print(\"taking target\")\n",
    "    target = targets[-1]\n",
    "    \n",
    "    print(f'making output for target layer {target}')\n",
    "    \n",
    "    get_output = tf.keras.Model(inputs=model.input, \n",
    "                                outputs=[model.layers[target - 1].output,\n",
    "                                         model.layers[target].output])\n",
    "    \n",
    "    print(f'making replacement layers for target layer {target}')\n",
    "    replacement_layers = build_replacement(get_output)\n",
    "    \n",
    "    replacement_len = len(replacement_layers.layers)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "\n",
    "    loss_object = tf.losses.MeanSquaredError()\n",
    "    \n",
    "    replacement_layers.compile(loss=loss_object, optimizer=optimizer)\n",
    "    \n",
    "    save = tf.keras.callbacks.ModelCheckpoint('./replacement_layer.h5', \n",
    "                                              verbose=0, \n",
    "                                              save_weights_only=True,\n",
    "                                              save_best_only=True)\n",
    "    train_gen = LayerBatch(get_output, dataset)\n",
    "    test_gen = LayerTest(get_output, dataset_test)\n",
    "    \n",
    "    print(f'starting fit generator for target layer {target}')\n",
    "#     replacement_layers.fit_generator(generator=train_gen, \n",
    "#                                      epochs=50, \n",
    "#                                      validation_data=test_gen ,\n",
    "#                                      verbose=0, callbacks=[save])\n",
    "    \n",
    "    print('saving replacement layers to json')\n",
    "    \n",
    "    replacement_json = replacement_layers.to_json()\n",
    "    with open('replacement_layer.json', 'w') as json_file:\n",
    "        json_file.write(replacement_json)\n",
    "        \n",
    "    del replacement_layers\n",
    "    \n",
    "    with open('replacement_layer.json', 'r') as json_file:\n",
    "        replacement_layers = tf.keras.models.model_from_json(json_file.read())\n",
    "\n",
    "    print('loading replacement layers weights')\n",
    "    replacement_layers.load_weights('replacement_layer.h5')\n",
    "    replacement_layers.compile(loss=loss_object, optimizer=optimizer)\n",
    "    replacement_layers.evaluate_generator(test_gen)\n",
    "    # build top half of model\n",
    "    print('building top half of model')\n",
    "    get_output = tf.keras.Model(inputs=model.input, outputs=[model.layers[target - 1].output])\n",
    "    # add in replacement layers\n",
    "    print('building middle of model with replacement layers')\n",
    "    new_joint = tf.keras.Model(inputs=get_output.input, outputs=replacement_layers(get_output.output))\n",
    "    \n",
    "    # build bottom of model\n",
    "    bottom_half = tf.keras.Sequential()\n",
    "    for layer in model.layers[target + 1::]:\n",
    "        bottom_half.add(layer)\n",
    "    \n",
    "    print('building bottom of model')\n",
    "    bottom_half.build(input_shape=new_joint.output.shape)\n",
    "    print('combining model')\n",
    "    combined = tf.keras.Model(inputs=new_joint.input, outputs=bottom_half(new_joint.output))\n",
    "    \n",
    "    combined.layers[-1].trainable=False\n",
    "    opt = keras.optimizers.RMSprop(lr=0.00005, decay=1e-6)\n",
    "    combined.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    del bottom_half, new_joint, replacement_layers, model\n",
    "    \n",
    "    print('testing combined model')\n",
    "    scores = combined.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    new_combined = tf.keras.Sequential()\n",
    "    new_layers = []\n",
    "    new_combined.add(tf.keras.layers.Input(shape=(32,32,3)))\n",
    "    accum = 0\n",
    "    print('refactoring model')\n",
    "    for layer in combined.layers:\n",
    "        if hasattr(layer, 'layers'):\n",
    "            for sublayer in layer.layers:\n",
    "                if(sublayer.__class__.__name__ != 'InputLayer'): \n",
    "                    new_layers.append((sublayer.__class__.__name__, sublayer.get_config(), accum))\n",
    "                accum += 1\n",
    "        elif layer.__class__.__name__ != 'InputLayer':          \n",
    "            new_layers.append((layer.__class__.__name__, layer.get_config(), accum))\n",
    "            accum += 1 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i, layer in enumerate(new_layers):\n",
    "        new_combined.add(keras.layers.deserialize(\n",
    "                                {'class_name': layer[0], \n",
    "                                 'config': layer[1]}))\n",
    "\n",
    "    new_combined.build()\n",
    "\n",
    "    accum = 0\n",
    "    for i, layer in enumerate(combined.layers):\n",
    "        if hasattr(layer, 'layers'):\n",
    "\n",
    "            for sublayer in layer.layers:\n",
    "                #print(f'{accum} sub is {sublayer} new is {new_combined.layers[accum]}')\n",
    "                if(sublayer.__class__.__name__ != 'InputLayer'):              \n",
    "                    new_combined.layers[accum].set_weights(sublayer.get_weights())\n",
    "                    accum += 1\n",
    "    #             else:\n",
    "    #                 accum += 1\n",
    "            continue\n",
    "        else:\n",
    "            #print(layer)\n",
    "            new_combined.layers[accum].set_weights(layer.get_weights())\n",
    "            accum +=1 \n",
    "\n",
    "    print('freezing first half of layers')\n",
    "    for i in range(target):\n",
    "        new_combined.layers[i].trainable = False\n",
    "    \n",
    "    print('freezing last half of layers')\n",
    "    for i in range(target +  replacement_len, len(new_combined.layers)):\n",
    "        new_combined.layers[i].trainable = False\n",
    "        \n",
    "    new_combined.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    del combined\n",
    "    gc.collect()\n",
    "\n",
    "    new_save=tf.keras.callbacks.ModelCheckpoint('./refactor_finetune.h5', \n",
    "                                                verbose=1, \n",
    "                                                save_weights_only=False, \n",
    "                                                save_best_only=True)\n",
    "    print('fine tuning combined model')\n",
    "    #new_combined.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=5, callbacks=[new_save])\n",
    "#     new_combined.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=5,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         #workers=5,\n",
    "#                         callbacks=[new_save])\n",
    "    \n",
    "    print('loading best weights from fine tune')\n",
    "    new_combined.load_weights('./refactor_finetune.h5')\n",
    "    \n",
    "    model = new_combined\n",
    "    del new_combined\n",
    "    print('new summary')\n",
    "    model.summary()\n",
    "    targets = [i for i, layer in enumerate(model.layers) if layer.__class__.__name__ == 'Conv2D']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
